"""
Vulnerability Analysis Agent for autonomous security assessment.
Specializes in CVE analysis, risk assessment, and vulnerability management.
"""

import logging
from typing import Dict, Any, List
import json
import re
from datetime import datetime, timedelta

from langchain.schema import HumanMessage, AIMessage
from langchain.tools import Tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor

from .base_agent import BaseAgent, AgentState
from ..routing import IntelligentRouteEngine
from config.settings import settings

logger = logging.getLogger(__name__)


class VulnerabilityAgent(BaseAgent):
    """
    Specialized agent for vulnerability analysis and security risk assessment.
    Focuses on CVE analysis, exploit assessment, and remediation guidance.
    """
    
    def __init__(self, route_engine: IntelligentRouteEngine):
        super().__init__(
            name="VulnerabilityAgent", 
            description="Expert in vulnerability analysis, CVE research, and security risk assessment"
        )
        self.route_engine = route_engine
        self.tools = self._initialize_tools()
        self.tool_executor = ToolExecutor(self.tools)
        
    def _initialize_tools(self) -> List[Tool]:
        """Initialize specialized tools for vulnerability analysis."""
        return [
            Tool(
                name="analyze_cve",
                description="Analyze specific CVE vulnerabilities with detailed technical assessment. Input should be CVE ID or vulnerability description.",
                func=self._analyze_cve
            ),
            Tool(
                name="assess_exploit_risk",
                description="Assess exploit availability and weaponization risk for vulnerabilities. Input should be CVE or vulnerability details.",
                func=self._assess_exploit_risk
            ),
            Tool(
                name="evaluate_patch_priority",
                description="Evaluate patching priority based on risk factors and business impact. Input should be vulnerability and environment context.",
                func=self._evaluate_patch_priority
            ),
            Tool(
                name="search_similar_vulnerabilities",
                description="Search for similar vulnerabilities and attack patterns. Input should be vulnerability characteristics or affected products.",
                func=self._search_similar_vulnerabilities
            ),
            Tool(
                name="generate_risk_report",
                description="Generate comprehensive vulnerability risk report. Input should be vulnerability data and organizational context.",
                func=self._generate_risk_report
            )
        ]
    
    def create_graph(self) -> StateGraph:
        """Create the vulnerability analysis workflow graph."""
        graph = StateGraph(AgentState)
        
        # Define nodes
        graph.add_node("analyze_query", self._analyze_vulnerability_query)
        graph.add_node("gather_vulnerability_data", self._gather_vulnerability_data)
        graph.add_node("assess_risk_factors", self._assess_risk_factors)
        graph.add_node("analyze_exploitability", self._analyze_exploitability)
        graph.add_node("evaluate_impact", self._evaluate_impact)
        graph.add_node("generate_recommendations", self._generate_recommendations)
        graph.add_node("validate_results", self.validate_results)
        graph.add_node("handle_error", self._handle_agent_error)
        
        # Define edges
        graph.set_entry_point("analyze_query")
        
        graph.add_conditional_edges(
            "analyze_query",
            self._should_continue,
            {
                "gather_data": "gather_vulnerability_data",
                "error": "handle_error",
                "end": END
            }
        )
        
        graph.add_edge("gather_vulnerability_data", "assess_risk_factors")
        graph.add_edge("assess_risk_factors", "analyze_exploitability")
        graph.add_edge("analyze_exploitability", "evaluate_impact")
        graph.add_edge("evaluate_impact", "generate_recommendations")
        graph.add_edge("generate_recommendations", "validate_results")
        
        graph.add_conditional_edges(
            "validate_results",
            self._should_retry,
            {
                "retry": "gather_vulnerability_data",
                "end": END,
                "error": "handle_error"
            }
        )
        
        graph.add_edge("handle_error", END)
        
        return graph
    
    async def _analyze_vulnerability_query(self, state: AgentState) -> AgentState:
        """Analyze the query to determine vulnerability analysis approach."""
        query = state["query"]
        
        # Extract CVE IDs if present
        cve_ids = re.findall(r'CVE-\d{4}-\d{4,}', query, re.IGNORECASE)
        
        # Determine analysis type
        analysis_type = self._determine_analysis_type(query)
        
        prompt = self.create_prompt_template("""
        Analyze this vulnerability-related query and create an analysis plan:
        
        Query: {query}
        Detected CVEs: {cve_ids}
        Analysis Type: {analysis_type}
        
        Create a structured analysis plan including:
        1. Primary objectives (CVE research, risk assessment, impact analysis)
        2. Required data sources (vulnerability databases, exploit feeds, patch information)
        3. Risk assessment approach (CVSS analysis, exploit availability, business impact)
        4. Expected deliverables (technical analysis, risk rating, recommendations)
        
        Respond with a JSON-formatted analysis plan.
        """)
        
        try:
            response = await self.llm.ainvoke(
                prompt.format_messages(
                    query=query,
                    cve_ids=str(cve_ids),
                    analysis_type=analysis_type
                )
            )
            
            analysis_plan = self._parse_llm_response(response.content)
            
            state["metadata"]["analysis_plan"] = analysis_plan
            state["context"]["cve_ids"] = cve_ids
            state["context"]["analysis_type"] = analysis_type
            state["context"]["vulnerability_focus"] = self._extract_vulnerability_focus(query)
            
            logger.info(f"Vulnerability query analysis completed: {analysis_type}")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    async def _gather_vulnerability_data(self, state: AgentState) -> AgentState:
        """Gather comprehensive vulnerability data."""
        query = state["query"]
        cve_ids = state["context"].get("cve_ids", [])
        
        try:
            # Use routing engine for primary data
            route_result = await self.route_engine.route_query(query, state["context"])
            
            vulnerability_data = {
                "primary_data": route_result.data,
                "source": route_result.source,
                "confidence": route_result.confidence_score,
                "cve_details": {}
            }
            
            # Enhance with specific CVE analysis if CVE IDs found
            for cve_id in cve_ids:
                cve_analysis = await self._analyze_cve(cve_id)
                vulnerability_data["cve_details"][cve_id] = cve_analysis
            
            # Add exploit intelligence
            if route_result.data:
                exploit_assessment = await self._assess_exploit_risk(
                    json.dumps(route_result.data[:3], default=str)
                )
                vulnerability_data["exploit_assessment"] = exploit_assessment
            
            state["context"]["vulnerability_data"] = vulnerability_data
            logger.info(f"Vulnerability data gathering completed: {len(route_result.data)} records")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    async def _assess_risk_factors(self, state: AgentState) -> AgentState:
        """Assess various risk factors for the vulnerabilities."""
        vulnerability_data = state["context"].get("vulnerability_data", {})
        
        try:
            risk_prompt = self.create_prompt_template("""
            Assess risk factors for these vulnerabilities:
            
            Vulnerability Data: {vulnerability_data}
            
            Analyze these risk factors:
            1. CVSS Scores and Severity Levels
            2. Attack Vector and Complexity
            3. Privileges Required and User Interaction
            4. Affected System Components (Confidentiality, Integrity, Availability)
            5. Exploit Maturity and Public Availability
            6. Affected Software Prevalence
            7. Mitigation Availability (patches, workarounds)
            
            Provide structured risk assessment with:
            - Individual vulnerability risk scores (1-10)
            - Aggregate risk rating (Critical/High/Medium/Low)
            - Risk factor breakdown and justification
            - Time-sensitive vulnerabilities identification
            
            Format as JSON with detailed risk analysis.
            """)
            
            vulnerability_summary = json.dumps({
                "record_count": len(vulnerability_data.get("primary_data", [])),
                "cve_count": len(vulnerability_data.get("cve_details", {})),
                "source": vulnerability_data.get("source", "unknown"),
                "sample_data": vulnerability_data.get("primary_data", [])[:2]
            }, default=str)
            
            response = await self.llm.ainvoke(
                risk_prompt.format_messages(vulnerability_data=vulnerability_summary)
            )
            
            risk_assessment = self._parse_llm_response(response.content)
            state["context"]["risk_assessment"] = risk_assessment
            
            logger.info("Risk factor assessment completed")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    async def _analyze_exploitability(self, state: AgentState) -> AgentState:
        """Analyze exploit availability and weaponization potential."""
        vulnerability_data = state["context"].get("vulnerability_data", {})
        risk_assessment = state["context"].get("risk_assessment", {})
        
        try:
            exploit_prompt = self.create_prompt_template("""
            Analyze exploitability for these vulnerabilities:
            
            Risk Assessment: {risk_assessment}
            Vulnerability Details: {vulnerability_details}
            
            Evaluate:
            1. Public Exploit Availability (PoCs, working exploits, exploit kits)
            2. Weaponization Level (script kiddie, sophisticated attacker, nation-state)
            3. Attack Prerequisites (network access, authentication, user interaction)
            4. Exploitation Complexity (technical skill required, automation potential)
            5. Detection Difficulty (stealth, persistence, forensic artifacts)
            6. Chaining Potential (multi-stage attacks, privilege escalation)
            
            Provide exploitability analysis including:
            - Exploit timeline (when first exploits appeared)
            - Current threat level (active exploitation evidence)
            - Expected exploitation trajectory
            - Defensive posture recommendations
            
            Format as structured exploitability assessment.
            """)
            
            risk_summary = json.dumps(risk_assessment, default=str)[:1000]
            vuln_summary = json.dumps({
                "exploit_data": vulnerability_data.get("exploit_assessment", {}),
                "cve_details": list(vulnerability_data.get("cve_details", {}).keys())
            }, default=str)
            
            response = await self.llm.ainvoke(
                exploit_prompt.format_messages(
                    risk_assessment=risk_summary,
                    vulnerability_details=vuln_summary
                )
            )
            
            exploitability_analysis = self._parse_llm_response(response.content)
            state["context"]["exploitability_analysis"] = exploitability_analysis
            
            logger.info("Exploitability analysis completed")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    async def _evaluate_impact(self, state: AgentState) -> AgentState:
        """Evaluate business and technical impact of vulnerabilities."""
        risk_assessment = state["context"].get("risk_assessment", {})
        exploitability = state["context"].get("exploitability_analysis", {})
        vulnerability_data = state["context"].get("vulnerability_data", {})
        
        try:
            impact_prompt = self.create_prompt_template("""
            Evaluate the comprehensive impact of these vulnerabilities:
            
            Risk Assessment: {risk_assessment}
            Exploitability Analysis: {exploitability}
            
            Analyze impact across:
            1. Technical Impact (system compromise, data access, service disruption)
            2. Business Impact (operational disruption, financial loss, reputation damage)
            3. Compliance Impact (regulatory violations, audit findings)
            4. Strategic Impact (competitive advantage loss, IP theft)
            5. Cascading Effects (supply chain, partner networks, customer trust)
            
            Consider:
            - Affected asset criticality
            - Data sensitivity and classification
            - Service availability requirements
            - Recovery time and cost estimates
            - Long-term consequences
            
            Provide structured impact assessment with severity ratings and justifications.
            """)
            
            risk_summary = json.dumps(risk_assessment, default=str)[:1000]
            exploit_summary = json.dumps(exploitability, default=str)[:1000]
            
            response = await self.llm.ainvoke(
                impact_prompt.format_messages(
                    risk_assessment=risk_summary,
                    exploitability=exploit_summary
                )
            )
            
            impact_assessment = self._parse_llm_response(response.content)
            state["context"]["impact_assessment"] = impact_assessment
            
            logger.info("Impact assessment completed")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    async def _generate_recommendations(self, state: AgentState) -> AgentState:
        """Generate comprehensive vulnerability management recommendations."""
        query = state["query"]
        risk_assessment = state["context"].get("risk_assessment", {})
        exploitability = state["context"].get("exploitability_analysis", {})
        impact_assessment = state["context"].get("impact_assessment", {})
        vulnerability_data = state["context"].get("vulnerability_data", {})
        
        try:
            recommendations_prompt = self.create_prompt_template("""
            Generate comprehensive vulnerability management recommendations:
            
            Original Query: {query}
            Risk Assessment: {risk_assessment}
            Exploitability: {exploitability}
            Impact Assessment: {impact_assessment}
            
            Provide actionable recommendations for:
            
            1. IMMEDIATE ACTIONS (0-24 hours)
            - Critical vulnerabilities requiring emergency patching
            - Temporary mitigations and compensating controls
            - Threat hunting and incident response preparation
            
            2. SHORT-TERM ACTIONS (1-30 days)
            - Patch deployment prioritization and scheduling
            - System hardening and configuration changes
            - Monitoring and detection rule implementation
            
            3. LONG-TERM ACTIONS (30+ days)
            - Vulnerability management process improvements
            - Architecture and technology stack modifications
            - Security awareness and training programs
            
            4. STRATEGIC RECOMMENDATIONS
            - Risk acceptance decisions and business justification
            - Resource allocation and budget planning
            - Third-party risk management considerations
            
            Format as prioritized action plan with timelines, owners, and success metrics.
            """)
            
            response = await self.llm.ainvoke(
                recommendations_prompt.format_messages(
                    query=query,
                    risk_assessment=json.dumps(risk_assessment, default=str)[:800],
                    exploitability=json.dumps(exploitability, default=str)[:800], 
                    impact_assessment=json.dumps(impact_assessment, default=str)[:800]
                )
            )
            
            # Structure the final results
            analysis_results = [
                {
                    "type": "vulnerability_analysis",
                    "query": query,
                    "executive_summary": response.content,
                    "risk_assessment": risk_assessment,
                    "exploitability_analysis": exploitability,
                    "impact_assessment": impact_assessment,
                    "data_sources": vulnerability_data.get("source", "unknown"),
                    "confidence": vulnerability_data.get("confidence", 0.0),
                    "cve_count": len(vulnerability_data.get("cve_details", {})),
                    "timestamp": state["metadata"]["start_time"]
                }
            ]
            
            # Add detailed vulnerability data as supporting evidence
            if vulnerability_data.get("primary_data"):
                analysis_results.extend(vulnerability_data["primary_data"][:15])
            
            state["results"] = analysis_results
            logger.info(f"Vulnerability analysis completed: {len(analysis_results)} results")
            
        except Exception as e:
            state = await self.handle_error(state, e)
        
        return state
    
    def _determine_analysis_type(self, query: str) -> str:
        """Determine the type of vulnerability analysis needed."""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["cve-", "vulnerability id"]):
            return "cve_research"
        elif any(word in query_lower for word in ["risk", "assess", "priority"]):
            return "risk_assessment"
        elif any(word in query_lower for word in ["exploit", "attack", "weaponiz"]):
            return "exploit_analysis"
        elif any(word in query_lower for word in ["patch", "fix", "remediat", "mitigat"]):
            return "remediation_guidance"
        elif any(word in query_lower for word in ["impact", "business", "consequenc"]):
            return "impact_analysis"
        else:
            return "general_vulnerability_analysis"
    
    def _extract_vulnerability_focus(self, query: str) -> Dict[str, Any]:
        """Extract specific vulnerability focus areas from the query."""
        focus = {
            "products": [],
            "vendors": [],
            "severity_levels": [],
            "timeframe": None,
            "attack_vectors": []
        }
        
        # Extract vendor/product information
        vendor_patterns = [
            r'\b(?:microsoft|windows|adobe|oracle|cisco|vmware|apache|nginx)\b',
            r'\bvendor:\s*([^\s,]+)',
            r'\bcompany:\s*([^\s,]+)'
        ]
        
        for pattern in vendor_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            focus["vendors"].extend(matches)
        
        # Extract severity levels
        severity_matches = re.findall(r'\b(critical|high|medium|low)\b', query, re.IGNORECASE)
        focus["severity_levels"] = [s.lower() for s in severity_matches]
        
        # Extract timeframe
        if re.search(r'\b(recent|latest|new|emerging)\b', query, re.IGNORECASE):
            focus["timeframe"] = "recent"
        elif re.search(r'\b(\d{4})\b', query):
            year_match = re.search(r'\b(\d{4})\b', query)
            focus["timeframe"] = year_match.group(1)
        
        return focus
    
    async def _analyze_cve(self, cve_input: str) -> Dict[str, Any]:
        """Analyze specific CVE vulnerability."""
        return {
            "analysis_type": "cve_analysis",
            "input": cve_input,
            "results": f"CVE analysis for: {cve_input}",
            "confidence": 0.9
        }
    
    async def _assess_exploit_risk(self, vulnerability_data: str) -> Dict[str, Any]:
        """Assess exploit risk for vulnerabilities."""
        return {
            "analysis_type": "exploit_risk",
            "input": vulnerability_data[:100] + "..." if len(vulnerability_data) > 100 else vulnerability_data,
            "results": "Exploit risk assessment completed",
            "confidence": 0.8
        }
    
    async def _evaluate_patch_priority(self, context: str) -> Dict[str, Any]:
        """Evaluate patching priority."""
        return {
            "analysis_type": "patch_priority",
            "input": context,
            "results": "Patch priority evaluation completed",
            "confidence": 0.8
        }
    
    async def _search_similar_vulnerabilities(self, characteristics: str) -> Dict[str, Any]:
        """Search for similar vulnerabilities."""
        return {
            "analysis_type": "similar_vulnerabilities",
            "input": characteristics,
            "results": "Similar vulnerability search completed",
            "confidence": 0.7
        }
    
    async def _generate_risk_report(self, context: str) -> Dict[str, Any]:
        """Generate comprehensive risk report."""
        return {
            "analysis_type": "risk_report",
            "input": context,
            "results": "Risk report generation completed",
            "confidence": 0.9
        }
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response, handling both JSON and text formats."""
        try:
            if response.strip().startswith('{'):
                return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        return {
            "content": response,
            "parsed": False,
            "format": "text"
        }
    
    def _should_continue(self, state: AgentState) -> str:
        """Determine if processing should continue."""
        if state.get("error"):
            return "error"
        
        if not state.get("query"):
            return "end"
        
        return "gather_data"
    
    def _should_retry(self, state: AgentState) -> str:
        """Determine if processing should retry."""
        if state.get("error") and state.get("retry_count", 0) < state.get("max_retries", 3):
            return "retry"
        
        if state.get("error"):
            return "error"
        
        return "end"
    
    async def _handle_agent_error(self, state: AgentState) -> AgentState:
        """Handle agent-specific errors."""
        error = state.get("error", "Unknown error")
        retry_count = state.get("retry_count", 0)
        
        logger.error(f"VulnerabilityAgent error (attempt {retry_count}): {error}")
        
        # Provide fallback results
        state["results"] = [{
            "type": "error_response",
            "message": f"Vulnerability analysis failed: {error}",
            "retry_count": retry_count,
            "fallback_available": False
        }]
        
        state["confidence"] = 0.0
        return state
    
    async def process_query(self, state: AgentState) -> AgentState:
        """
        Process query using the vulnerability analysis workflow.
        Called by the base class execute method.
        """
        return state